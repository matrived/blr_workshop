{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize Qwen-Image with DiffSynth-Studio\n",
    "\n",
    "This tutorial explores the capabilities of the Qwen-Image series—a massive 86B parameter model collection—and explains how to fine-tune it efficiently using DiffSynth-Studio on AMD hardware. It demonstrates how the high memory capacity of the AMD Instinct™ MI300X GPU enables loading multiple large models simultaneously for complex workflows involving inference, editing, and training.\n",
    "\n",
    "Note: This tutorial was developed by ModelScope and Tongyi Lab(Alibaba Group)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key components\n",
    "Hardware: AMD Instinct MI300X GPU\n",
    "\n",
    "Software: DiffSynth-Studio and ROCm\n",
    "\n",
    "Models: Qwen-Image, Qwen-Image-Edit, and Custom LoRA adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Before starting, ensure your environment meets the following requirements:\n",
    "\n",
    "**Operating system**: Linux (Ubuntu 22.04 recommended). See the official requirements for supported operating systems.\n",
    "\n",
    "**Hardware**: AMD Instinct MI300X GPU\n",
    "\n",
    "**Software**: ROCm 6.0 or later, Docker, and Python 3.10 or later\n",
    "\n",
    "**Note**: Install and verify ROCm by following the ROCm install guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "\n",
    "## Step 1: Environment setup\n",
    "\n",
    "### Verify the hardware availability\n",
    "\n",
    "The AMD Instinct MI300X GPU is designed to deliver peak performance for Generative AI workloads. Before you begin, verify that your GPU is correctly detected and ready for use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!amd-smi\n",
    "#For ROCm 6.4 and earlier, run rocm-smi instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install DiffSynth-Studio from source\n",
    "To ensure full compatibility with AMD ROCm, install DiffSynth-Studio directly from the source.\n",
    "\n",
    "**Note**: After installation, manually update the system path to ensure the notebook can import the library immediately without a kernel restart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Clone the repository\n",
    "!git clone https://github.com/modelscope/DiffSynth-Studio.git\n",
    "\n",
    "# 2. Navigate into the directory\n",
    "os.chdir(\"DiffSynth-Studio\")\n",
    "\n",
    "# 3. Checkout the specific commit for reproducibility\n",
    "!git checkout afd101f3452c9ecae0c87b79adfa2e22d65ffdc3\n",
    "\n",
    "# 4. Create the AMD-specific requirements file\n",
    "requirements_content = \"\"\"\n",
    "# Index for AMD ROCm 6.4 wheels (Prioritized)\n",
    "--index-url https://download.pytorch.org/whl/rocm6.4\n",
    "# Fallback to standard PyPI for all other libraries\n",
    "--extra-index-url https://pypi.org/simple\n",
    "# Core PyTorch libraries\n",
    "torch>=2.0.0\n",
    "torchvision\n",
    "transformers>=4.37.0\n",
    "# Install the DiffSynth-Studio project and its other dependencies\n",
    "-e .\n",
    "\"\"\".strip()\n",
    "\n",
    "with open(\"requirements-amd.txt\", \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "\n",
    "# 5. Install using the custom requirements\n",
    "!pip install -r requirements-amd.txt\n",
    "\n",
    "# 6. Force the current notebook to see the installed package\n",
    "sys.path.append(os.getcwd())\n",
    "print(f\"Added {os.getcwd()} to system path to enable immediate import.\")\n",
    "\n",
    "# 7. Return to root directory\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "\n",
    "## Step 2: Basic model inference\n",
    "\n",
    "This section demonstrates how to conduct inference with the model. \n",
    "\n",
    "Qwen-Image is a large-scale image generation model. Configure the pipeline and load the model components (Transformer, Text Encoder, and VAE) onto the GPU.\n",
    "\n",
    "**Note**: Configure the environment to use ModelScope as the domain for downloading weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.CRITICAL)\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Use HuggingFace cache (models already pre-downloaded by DaemonSet)\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/root/.cache/huggingface\"\nprint(\"✅ Using pre-downloaded models from /root/.cache/huggingface\")\n\nfrom diffsynth.pipelines.qwen_image import QwenImagePipeline, ModelConfig\nimport torch\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\n\n# Load models from HuggingFace cache (no download needed!)\nqwen_image = QwenImagePipeline.from_pretrained(\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n    model_configs=[\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n    ],\n    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n)\nqwen_image.enable_lora_magic()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a baseline image\n",
    "Generate your first image using the simple prompt: “a portrait of a beautiful Asian woman\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a beautiful Asian woman\"\n",
    "image = qwen_image(prompt, seed=0, num_inference_steps=40)\n",
    "image.resize((512, 512))\n",
    "# There might be error messages output, but they can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "\n",
    "## Step 3: Enhancing quality with LoRA\n",
    "\n",
    "You might notice that the baseline image lacks fine details.\n",
    "\n",
    "To improve the image, load Qwen-Image-LoRA-ArtAug-v1 to significantly enhance visual fidelity and artistic details in the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image.load_lora(\n",
    "    qwen_image.dit,\n",
    "    ModelConfig(model_id=\"DiffSynth-Studio/Qwen-Image-LoRA-ArtAug-v1\", origin_file_pattern=\"model.safetensors\"),\n",
    "    hotload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the same prompt to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"a portrait of a beautiful Asian woman\"\n",
    "image = qwen_image(prompt, seed=0, num_inference_steps=40)\n",
    "image.save(\"image_face.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "\n",
    "## Step 4: Multilingual and multi-image editing\n",
    "\n",
    "The Qwen-Image text encoder is robust enough to understand prompts in languages it wasn’t explicitly trained on. To try this out, generate a character using a Korean language prompt. \n",
    "\n",
    "First, generate an image using English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"A handsome Asian man wearing a dark gray slim-fit suit, with calm, smiling eyes that exude confidence and composure. He is seated at a table, holding a bouquet of red flowers in his hands.\"\n",
    "image = qwen_image(prompt, seed=2, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use Korean to determine whether the model can understand the image content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"잘생긴 아시아 남성으로, 짙은 회색의 슬림핏 수트를 입고 있으며, 침착하면서도 미소를 머금은 눈빛으로 자신감 있고 여유로운 분위기를 풍긴다. 그는 책상 앞에 앉아 붉은 꽃다발을 손에 들고 있다.\"\n",
    "image = qwen_image(prompt, seed=2, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Kannada language to determine whether the model can understand the image content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image.clear_lora()\n",
    "prompt = \"ಗಾಢ ಬೂದು ಬಣ್ಣದ ಸ್ಲಿಮ್-ಫಿಟ್ ಸೂಟ್ ಧರಿಸಿದ, ಆತ್ಮವಿಶ್ವಾಸ ಮತ್ತು ಶಾಂತತೆಯನ್ನು ಹೊರಹಾಕುವ ಶಾಂತ, ನಗುತ್ತಿರುವ ಕಣ್ಣುಗಳನ್ನು ಹೊಂದಿರುವ ಒಬ್ಬ ಸುಂದರ ಏಷ್ಯನ್ ವ್ಯಕ್ತಿ. ಅವನು ಮೇಜಿನ ಬಳಿ ಕುಳಿತಿದ್ದಾನೆ, ಕೈಯಲ್ಲಿ ಕೆಂಪು ಹೂವುಗಳ ಪುಷ್ಪಗುಚ್ಛವನ್ನು ಹಿಡಿದಿದ್ದಾನೆ.\"\n",
    "image = qwen_image(prompt, seed=2, num_inference_steps=40)\n",
    "image.save(\"image_man.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Qwen-Image wasn’t trained on Korean or Kannada text, the foundational capabilities of its text encoder still provide multilingual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some clean-up for the models we do not need anymore. However, lets see model with how many parameters we have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "qwen_image_params = count_parameters(qwen_image)\n",
    "print(qwen_image_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "del qwen_image\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "\n",
    "## Step 5: Advanced image editing\n",
    "\n",
    "This section describes some advanced techniques for producing more complex images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the editing pipeline\n",
    "\n",
    "The Qwen-Image series includes specialized models for different tasks. Next, load Qwen-Image-Edit, a model designed specifically for image editing and in-painting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "# Load editing pipeline from pre-downloaded HuggingFace cache\nqwen_image_edit = QwenImagePipeline.from_pretrained(\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n    model_configs=[\n        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n    ],\n    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n    processor_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"processor/\"),\n)\nqwen_image_edit.enable_lora_magic()\nprint(\"✅ Loaded Qwen-Image-Edit from cache (no download)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outpainting with consistency\n",
    "\n",
    "You can perform an outpainting task by taking the portrait you just generated and extending it into a long-shot image with a forest background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Realistic photography of a beautiful woman wearing a long dress. The background is a forest.\"\n",
    "negative_prompt = \"Make the character's fingers mutilated and distorted, enlarge the head to create an unnatural head-to-body ratio, turning the figure into a short-statured big-headed doll. Generate harsh, glaring sunlight and render the entire scene with oversaturated colors. Twist the legs into either X-shaped or O-shaped deformities.\"\n",
    "image = qwen_image_edit(prompt, negative_prompt=negative_prompt, edit_image=Image.open(\"image_face.jpg\"), seed=1, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The faces in this photo appear inconsistent. Load the specialized LoRA model DiffSynth-Studio/Qwen-Image-Edit-F2P that can generate consistent images based on facial references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image_edit.load_lora(\n",
    "    qwen_image_edit.dit,\n",
    "    ModelConfig(model_id=\"DiffSynth-Studio/Qwen-Image-Edit-F2P\", origin_file_pattern=\"model.safetensors\"),\n",
    "    hotload=True,\n",
    ")\n",
    "prompt = \"Realistic photography of a beautiful woman wearing a long dress. The background is a forest.\"\n",
    "negative_prompt = \"Make the character's fingers mutilated and distorted, enlarge the head to create an unnatural head-to-body ratio, turning the figure into a short-statured big-headed doll. Generate harsh, glaring sunlight and render the entire scene with oversaturated colors. Twist the legs into either X-shaped or O-shaped deformities.\"\n",
    "image = qwen_image_edit(prompt, negative_prompt=negative_prompt, edit_image=Image.open(\"image_face.jpg\"), seed=1, num_inference_steps=40)\n",
    "image.save(\"image_fullbody.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some clean-up for the models we do not need anymore. However, lets see model with how many parameters we have loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "qwen_image_edit_params = count_parameters(qwen_image_edit)\n",
    "print(qwen_image_edit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "del qwen_image_edit\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging subjects with Qwen-Image-Edit-2509\n",
    "\n",
    "You now have two images: the woman in the forest and the man with flowers. Using Qwen-Image-Edit-2509, which supports multi-image editing, you can merge these two independent images into a single cohesive scene where the characters are interacting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "# Load multi-image editing pipeline from pre-downloaded HuggingFace cache\nqwen_image_edit_2509 = QwenImagePipeline.from_pretrained(\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n    model_configs=[\n        ModelConfig(model_id=\"Qwen/Qwen-Image-Edit-2509\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n    ],\n    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n    processor_config=ModelConfig(model_id=\"Qwen/Qwen-Image-Edit\", origin_file_pattern=\"processor/\"),\n)\nqwen_image_edit_2509.enable_lora_magic()\nprint(\"✅ Loaded Qwen-Image-Edit-2509 from cache (no download)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate a photo of these two people together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"이 사랑 넘치는 부부의 포옹하는 모습을 찍은 사진을 생성해 줘.\"\n",
    "image = qwen_image_edit_2509(prompt, edit_image=[Image.open(\"image_fullbody.jpg\"), Image.open(\"image_man.jpg\")], seed=3, num_inference_steps=40)\n",
    "image.save(\"image_merged.jpg\")\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum([p.numel() for p in model.parameters()])\n",
    "\n",
    "qwen_image_edit_2509_params = count_parameters(qwen_image_edit_2509)\n",
    "print(qwen_image_edit_2509_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "del qwen_image_edit_2509\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Total parameters from all 3 models:\n",
    "\n",
    "total_params = qwen_image_params + qwen_image_edit_params + qwen_image_edit_2509_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "\n",
    "## Step 6: The power of the Instinct MI300X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Parameters**: ~86 Billion\n",
    "\n",
    "**Important Note**: Please note that all 3 of the models can be loaded simultaneously on a single AMD Instinct MI300X GPU which has 192 GB of VRAM but for the simiplicity of the workshop, each of you are using a fixed dedicated VRAM.\n",
    "\n",
    "Handling all 3 models on a standard GPU would be impossible. However, the AMD Instinct MI300X GPU can keep all these models resident in memory for seamless switching between inference, editing, and training tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "\n",
    "## Step 7: Training a custom LoRA\n",
    "\n",
    "Finally, it’s time to move from inference to training. Train a custom LoRA adapter to teach the model a specific concept, in this case, a specific dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "Download a small dataset containing five images of a dog and the associated metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "dataset_snapshot_download(\"Artiprocher/dataset_dog\", allow_file_pattern=[\"*.jpg\", \"*.csv\"], local_dir=\"dataset\")\n",
    "images = [Image.open(f\"dataset/{i}.jpg\") for i in range(1, 6)]\n",
    "Image.fromarray(np.concatenate([np.array(image.resize((256, 256))) for image in images], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the metadata for this dataset, including annotated image descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(\"dataset/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Run the training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the official training script and launch it using the accelerate command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/modelscope/DiffSynth-Studio/raw/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3/examples/qwen_image/model_training/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cmd = rf\"\"\"\n",
    "accelerate launch train.py \\\n",
    "  --dataset_base_path dataset \\\n",
    "  --dataset_metadata_path dataset/metadata.csv \\\n",
    "  --max_pixels 1048576 \\\n",
    "  --dataset_repeat 50 \\\n",
    "  --model_id_with_origin_paths \"Qwen/Qwen-Image:transformer/diffusion_pytorch_model*.safetensors,Qwen/Qwen-Image:text_encoder/model*.safetensors,Qwen/Qwen-Image:vae/diffusion_pytorch_model.safetensors\" \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --num_epochs 1 \\\n",
    "  --remove_prefix_in_ckpt \"pipe.dit.\" \\\n",
    "  --output_path \"lora_dog\" \\\n",
    "  --lora_base_model \"dit\" \\\n",
    "  --lora_target_modules \"to_q,to_k,to_v,add_q_proj,add_k_proj,add_v_proj,to_out.0,to_add_out,img_mlp.net.2,img_mod.1,txt_mlp.net.2,txt_mod.1\" \\\n",
    "  --lora_rank 32 \\\n",
    "  --dataset_num_workers 2 \\\n",
    "  --find_unused_parameters\n",
    "\"\"\".strip()\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step8\"></a>\n",
    "\n",
    "## Step 8: Inference with the custom LoRA\n",
    "\n",
    "Now that training is complete, load the model again, inject the newly trained lora_dog, and verify that the model recognizes your specific dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "# Reload base model from pre-downloaded HuggingFace cache\nqwen_image = QwenImagePipeline.from_pretrained(\n    torch_dtype=torch.bfloat16,\n    device=\"cuda\",\n    model_configs=[\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"transformer/diffusion_pytorch_model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"text_encoder/model*.safetensors\"),\n        ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"vae/diffusion_pytorch_model.safetensors\"),\n    ],\n    tokenizer_config=ModelConfig(model_id=\"Qwen/Qwen-Image\", origin_file_pattern=\"tokenizer/\"),\n)\nqwen_image.enable_lora_magic()\nprint(\"✅ Loaded Qwen-Image from cache for LoRA inference (no download)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, reload the model and generate photos for the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "qwen_image.load_lora(\n",
    "    qwen_image.dit,\n",
    "    \"lora_dog/epoch-0.safetensors\",\n",
    "    hotload=True\n",
    ")\n",
    "prompt = \"a dog\"\n",
    "image = qwen_image(prompt, seed=3, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate another image of the dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"a dog is jumping.\"\n",
    "image = qwen_image(prompt, seed=3, num_inference_steps=40)\n",
    "image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id=\"Conclusion\"></a>\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This tutorial demonstrated the end-to-end capabilities of the AMD Instinct MI300X.\n",
    "\n",
    "You successfully performed inference using models with 86B collective parameters, edited images with high consistency, and trained a custom adapter, all on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next Steps**: Please don't forget to explore the real power of AMD GPUs with your free cloud credit when you sign-up for [AMD AI Developer Program](https://www.amd.com/en/developer/ai-dev-program.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}